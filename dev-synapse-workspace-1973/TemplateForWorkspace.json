{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "dev-synapse-workspace-1973"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=dev-synapse-workspace-1973.database.windows.net;Initial Catalog=master"
		},
		"DataLake_DEV_LS_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'DataLake_DEV_LS'"
		},
		"Test_data_lake_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'Test_data_lake'"
		},
		"dev-synapse-workspace-1973-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dev-synapse-workspace-1973-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:dev-synapse-workspace-1973.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"DataLake_DEV_LS_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://devsynapsedl2.dfs.core.windows.net/"
		},
		"Test_data_lake_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://testqasynapsedl2.dfs.core.windows.net/"
		},
		"dev-synapse-workspace-1973-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://devsynapsedl2.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Populate DB')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Description !!",
				"activities": [
					{
						"name": "CSVtoParquet",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "JsonSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "JsonReadSettings"
								}
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"path": "$['Id']"
										},
										"sink": {
											"name": "Id",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Name']"
										},
										"sink": {
											"name": "Name",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['RG']"
										},
										"sink": {
											"name": "RG",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['ApplicationId']"
										},
										"sink": {
											"name": "ApplicationId",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['ProjectName']"
										},
										"sink": {
											"name": "ProjectName",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['Application_Management_Group']"
										},
										"sink": {
											"name": "Application_Management_Group"
										}
									},
									{
										"source": {
											"path": "$['Tags']['DataRetention']"
										},
										"sink": {
											"name": "DataRetention",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['DeployDate']"
										},
										"sink": {
											"name": "DeployDate",
											"type": "Date"
										}
									},
									{
										"source": {
											"path": "$['Tags']['Support_Group']"
										},
										"sink": {
											"name": "Support_Group",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['Environment']"
										},
										"sink": {
											"name": "Environment",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['ProjectOwner']"
										},
										"sink": {
											"name": "ProjectOwner",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['TicketNo']"
										},
										"sink": {
											"name": "TicketNo",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['VM']"
										},
										"sink": {
											"name": "VM",
											"type": "String"
										}
									},
									{
										"source": {
											"path": "$['Tags']['DeployBy']"
										},
										"sink": {
											"name": "DeployBy",
											"type": "String"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "synapses_dev",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "synapses_parquet",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"variables": {
					"sssss": {
						"type": "String",
						"defaultValue": "ssssss"
					}
				},
				"annotations": [
					"ddddddd"
				],
				"lastPublishTime": "2023-07-06T12:49:22Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/synapses_dev')]",
				"[concat(variables('workspaceId'), '/datasets/synapses_parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake_DEV_LS",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Synapse.dev.out.txt",
						"fileSystem": "dev-synapse-fs"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake_DEV_LS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapses_dev')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake_DEV_LS",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Synapse.dev.json",
						"fileSystem": "dev-synapse-fs"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"Id": {
							"type": "string"
						},
						"Name": {
							"type": "string"
						},
						"RG": {
							"type": "string"
						},
						"Tags": {
							"type": "object",
							"properties": {
								"ApplicationId": {
									"type": "string"
								},
								"ProjectName": {
									"type": "string"
								},
								"Application_Management_Group": {
									"type": "string"
								},
								"DataRetention": {
									"type": "string"
								},
								"DeployDate": {
									"type": "string"
								},
								"Support_Group": {
									"type": "string"
								},
								"Environment": {
									"type": "string"
								},
								"ProjectOwner": {
									"type": "string"
								},
								"TicketNo": {
									"type": "string"
								},
								"VM": {
									"type": "string"
								},
								"DeployBy": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake_DEV_LS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapses_parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake_DEV_LS",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "synapses.parquet",
						"folderPath": "parquet",
						"fileSystem": "dev-synapse-fs"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake_DEV_LS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLake_DEV_LS')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('DataLake_DEV_LS_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('DataLake_DEV_LS_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Test_data_lake')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Test_data_lake_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('Test_data_lake_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dev-synapse-workspace-1973-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('dev-synapse-workspace-1973-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dev-synapse-workspace-1973-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dev-synapse-workspace-1973-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     store: 'synapse',",
						"     databaseType: 'spark',",
						"     format: 'table',",
						"     database: 'Database1',",
						"     tableName: 'Animal') ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DelimitedText1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/JSON_2_DB')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "synapses_dev",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          Id as string,",
						"          Name as string,",
						"          RG as string,",
						"          Tags as (ApplicationId as string, ProjectName as string, Application_Management_Group as string, DataRetention as string, DeployDate as string, Support_Group as string, Environment as string, ProjectOwner as string, TicketNo as string, VM as string, DeployBy as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'singleDocument') ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     store: 'synapse',",
						"     databaseType: 'spark',",
						"     format: 'table',",
						"     database: 'synapse_db',",
						"     tableName: 'synapses',",
						"     recreate:true,",
						"     mapColumn(",
						"          Id,",
						"          Name,",
						"          RG,",
						"          Tags",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/synapses_dev')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--- dev-1\nselect @@Version",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "lalalala",
				"content": {
					"query": "--- dev 2\nselect 1+1",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--- v3\n--- verify custom comments",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Strange looking file",
				"content": {
					"query": "--- script N4\nselect @@Version",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DIM_QTR_MNEMONIC_OFFSET_UPDATER')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Time"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run utils/utils-adls-credentials"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/utils-setup-logger"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# is_first_MMW = True"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update TIME.DIM_QTR_MNEMONIC_OFFSET table.\r\n",
							"\r\n",
							"# Column_Name\t            Update Frequency\r\n",
							"# DAILY_QTR_MNEMONIC_OFFSET\tFirst day of every quarter \r\n",
							"# QTR_MNEMONIC_OFFSET\t    First MMW of every quarter\r\n",
							"# CURR_QTR_INDC\t            First MMW of every quarter\r\n",
							"\r\n",
							"if is_first_MMW is None:\r\n",
							"    logger.error(\"is_first_MMW is mandatory input parameter\")\r\n",
							"    mssparkutils.notebook.exit(\"is_first_MMW is mandatory input parameter\")\r\n",
							"\r\n",
							"else:        \r\n",
							"    if is_first_MMW:\r\n",
							"        #update QTR_MNEMONIC_OFFSET and CURR_QTR_INDC\r\n",
							"        df_DIM_QTR_MNEMONIC_OFFSET = spark.sql(\"\"\"\r\n",
							"            select distinct\r\n",
							"            YEAR_QTR_SEQ_WED AS YEAR_QTR_SEQ_WED,\r\n",
							"            YEAR_QTR_SEQ as YEAR_QTR_SEQ,\r\n",
							"            DAILY_QTR_MNEMONIC_OFFSET AS DAILY_QTR_MNEMONIC_OFFSET,\r\n",
							"            cast((QTR_MNEMONIC_OFFSET -1) AS SMALLINT) AS QTR_MNEMONIC_OFFSET,\r\n",
							"            CURR_QTR_INDC AS CURR_QTR_INDC,\r\n",
							"            IW_ROW_UPDT_TS AS IW_ROW_UPDT_TS from time.DIM_QTR_MNEMONIC_OFFSET T\r\n",
							"        \"\"\")\r\n",
							"        df_DIM_QTR_MNEMONIC_OFFSET = df_DIM_QTR_MNEMONIC_OFFSET.withColumn('CURR_QTR_INDC', lit('N'))\r\n",
							"        df_DIM_QTR_MNEMONIC_OFFSET = df_DIM_QTR_MNEMONIC_OFFSET.withColumn(\"CURR_QTR_INDC\", when(df_DIM_QTR_MNEMONIC_OFFSET.QTR_MNEMONIC_OFFSET == 0, 'Y').otherwise('N'))\r\n",
							"        logger.info(\"Writing data into TIME.DIM_QTR_MNEMONIC_OFFSET table\")\r\n",
							"        df_DIM_QTR_MNEMONIC_OFFSET.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_MNEMONIC_OFFSET\")\r\n",
							"        logger.info(\"Load Completed: time.DIM_QTR_MNEMONIC_OFFSET\")\r\n",
							"    else:\r\n",
							"        #update DAILY_QTR_MNEMONIC_OFFSET\r\n",
							"        df_DIM_QTR_MNEMONIC_OFFSET = spark.sql(\"\"\"\r\n",
							"            select distinct\r\n",
							"            YEAR_QTR_SEQ_WED AS YEAR_QTR_SEQ_WED,\r\n",
							"            YEAR_QTR_SEQ as YEAR_QTR_SEQ,\r\n",
							"            cast((DAILY_QTR_MNEMONIC_OFFSET -1) AS SMALLINT) AS DAILY_QTR_MNEMONIC_OFFSET,\r\n",
							"            QTR_MNEMONIC_OFFSET AS QTR_MNEMONIC_OFFSET,\r\n",
							"            CURR_QTR_INDC AS CURR_QTR_INDC,\r\n",
							"            IW_ROW_UPDT_TS AS IW_ROW_UPDT_TS from time.DIM_QTR_MNEMONIC_OFFSET T\r\n",
							"        \"\"\")\r\n",
							"        logger.info(\"Writing data into TIME.DIM_QTR_MNEMONIC_OFFSET table\")\r\n",
							"        df_DIM_QTR_MNEMONIC_OFFSET.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_MNEMONIC_OFFSET\")\r\n",
							"        logger.info(\"Load Completed: time.DIM_QTR_MNEMONIC_OFFSET\")"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Time_Dimensions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Time"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#This notebook is to create REF_DAY_TIME.del file from REF_DAY_TIME_ESA.csv and load the del file into time.REF_DAY_TIME table.\r\n",
							"#Use time.REF_DAY_TIME table to populate other time dimension tables."
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/utils-adls-credentials"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/utils-setup-logger"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\r\n",
							"\r\n",
							"##init table from legacy\r\n",
							"def main():\r\n",
							"    logger.info(\"Starting Application ...\")\r\n",
							"    sparkAppName = \"CreateREF_DAY_TIMEdel\"\r\n",
							"    spark = SparkSession.builder.appName(sparkAppName).getOrCreate()\r\n",
							"    adls_host = token_library.getSecret(key_vault_name,\"ooo-cz-adls-host\",key_vault_linked_service_name)\r\n",
							"    basepath = \"/KYN_ISIP/time/raw/\"\r\n",
							"\r\n",
							"    # Reading REF_DAY_TIME_ESA.csv file and loading data into time.REF_DAY_TIME table\r\n",
							"    logger.info(\"Started to create REF_DAY_TIME.del file from  REF_DAY_TIME_ESA.csv : \\n\")\r\n",
							"    customSchema = StructType([ \r\n",
							"        StructField(\"DAY_PERIOD_KEY\",  IntegerType(), True),\r\n",
							"        StructField(\"DATE\", DateType(), True),\r\n",
							"        StructField(\"DAY_NUM_IN_WEEK\",  IntegerType(), True),\r\n",
							"        StructField(\"DAY_NUM_IN_MONTH\",  IntegerType(), True),\r\n",
							"        StructField(\"DAY_NUM_IN_QTR\",  IntegerType(), True),\r\n",
							"        StructField(\"DAY_NUM_IN_EPOCH\",  IntegerType(), True),\r\n",
							"        StructField(\"RPT_DAY_NUM_IN_QTR\",  IntegerType(), True),\r\n",
							"        StructField(\"DAY_OF_WEEK\", StringType(), True),\r\n",
							"        StructField(\"DAY_OF_WK_SHRT_NM\", StringType(), True),\r\n",
							"        StructField(\"RPT_WEEK_NUM_IN_MONTH\",  IntegerType(), True),\r\n",
							"        StructField(\"RPT_WEEK_NUM_IN_QTR\",  IntegerType(), True),\r\n",
							"        StructField(\"WEEK_NUM_IN_YEAR\",  IntegerType(), True),\r\n",
							"        StructField(\"WEEK_NUM_IN_EPOCH\",  IntegerType(), True),\r\n",
							"        StructField(\"MONTH_NUM_IN_QTR\",  IntegerType(), True),\r\n",
							"        StructField(\"MONTH_NUM_IN_YEAR\", StringType(), True),\r\n",
							"        StructField(\"MONTH_NUM_IN_EPOCH\",  IntegerType(), True),\r\n",
							"        StructField(\"MONTH_NAME\", StringType(), True),\r\n",
							"        StructField(\"MONTH_SHRT_NM\", StringType(), True),\r\n",
							"        StructField(\"QTR\", StringType(), True),\r\n",
							"        StructField(\"YEAR\", StringType(), True),\r\n",
							"        StructField(\"LOAD_RPT_QTR\", StringType(), True),\r\n",
							"        StructField(\"LOAD_RPT_YEAR\", StringType(), True),\r\n",
							"        StructField(\"WEEK_NUM_IN_QTR\",  IntegerType(), True),\r\n",
							"        StructField(\"YEAR_NUM\",  IntegerType(), True),\r\n",
							"        StructField(\"FIFTEENWK_WK_IN_QTR_NUM\",  IntegerType(), True),\r\n",
							"        StructField(\"FIFTEENWK_QTR_NUM\",  IntegerType(), True),\r\n",
							"        StructField(\"FIFTEENWK_QTR_CODE\", StringType(), True),\r\n",
							"        StructField(\"FIFTEENWK_YEAR_NUM\",  IntegerType(), True),\r\n",
							"        StructField(\"FIFTEENWK_YEAR\",  StringType(), True)\r\n",
							"    ])\r\n",
							"    logger.info(\"Reading REF_DAY_TIME_ESA.csv file..\")\r\n",
							"    ref_filename = adls_host + basepath + \"REF_DAY_TIME_ESA.csv\"\r\n",
							"    df = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(ref_filename)\r\n",
							"    df.write.mode(\"overwrite\").saveAsTable(\"time.REF_DAY_TIME\")\r\n",
							"    logger.info(\"Saved data into time.REF_DAY_TIME table\")\r\n",
							"\r\n",
							"    #Using time.REF_DAY_TIME to create REF_DAY_TIME.del for REF_DAY_TIME.del\r\n",
							"    logger.info(\"Started SQL logic to populate data for  REF_DAY_TIME.del file\")\r\n",
							"    df = spark.sql(\"\"\"\r\n",
							"            WITH GET_RPT_WEEK_NUM_IN_QTR AS (\r\n",
							"            SELECT AL1.DATE,\r\n",
							"            CASE WHEN AL2.MIN_DAY_NUM_IN_WEEK IN (4,5,6,7) THEN AL1.RPT_WEEK_NUM_IN_QTR - 1 ELSE AL1.RPT_WEEK_NUM_IN_QTR END AS RPT_WEEK_NUM_IN_QTR\r\n",
							"            FROM time.REF_DAY_TIME AL1\r\n",
							"            LEFT JOIN (\r\n",
							"            SELECT YEAR_NUM, QTR, MIN(DAY_NUM_IN_WEEK) MIN_DAY_NUM_IN_WEEK FROM \r\n",
							"            time.REF_DAY_TIME WHERE RPT_WEEK_NUM_IN_QTR = 1 AND MONTH_NUM_IN_QTR = 1 AND \r\n",
							"            DATE BETWEEN '1900-01-01' AND '2200-12-31' GROUP BY YEAR_NUM, QTR)  AL2 \r\n",
							"            ON AL1.YEAR_NUM = AL2.YEAR_NUM AND AL1.QTR = AL2.QTR\r\n",
							"            WHERE DAY_NUM_IN_WEEK = 3 AND DATE BETWEEN '1900-01-01' AND '2200-12-31' )\r\n",
							"            SELECT\r\n",
							"            RDT.DATE AS DAY_DATE,\r\n",
							"            RDT.DAY_NUM_IN_WEEK AS DAY_IN_WEEK_NUM,\r\n",
							"            RDT.DAY_NUM_IN_MONTH AS DAY_IN_MONTH_NUM,\r\n",
							"            RDT.DAY_NUM_IN_QTR AS DAY_IN_QTR_NUM,\r\n",
							"            RDT.DAY_OF_WEEK AS DAY_OF_WEEK_NAME,\r\n",
							"            RDT.DAY_OF_WK_SHRT_NM AS DAY_OF_WEEK_SHRT_NAME,\r\n",
							"            RDT.RPT_WEEK_NUM_IN_QTR AS WK_IN_QTR_NUM,\r\n",
							"            GT.RPT_WEEK_NUM_IN_QTR AS RPT_WK_IN_QTR_NUM,\r\n",
							"            WEEKOFYEAR(RDT.DATE) AS WK_IN_YEAR_NUM,\r\n",
							"            RDT.MONTH_NUM_IN_QTR AS MONTH_IN_QTR_NUM,\r\n",
							"            CAST(RDT.MONTH_NUM_IN_YEAR AS SMALLINT) AS MONTH_IN_YEAR_NUM,\r\n",
							"            CAST(RDT.MONTH_NUM_IN_YEAR AS SMALLINT) AS MONTH_IN_FISCAL_YEAR_NUM,\r\n",
							"            RDT.MONTH_NAME AS MONTH_NAME,\r\n",
							"            RDT.MONTH_SHRT_NM AS MONTH_SHRT_NAME,\r\n",
							"            CAST(SUBSTR(RDT.QTR,2,1) AS SMALLINT) AS QTR_NUM,\r\n",
							"            RDT.YEAR_NUM AS YEAR_NUM,\r\n",
							"            CAST(SUBSTR(RDT.QTR,2,1) AS SMALLINT) AS FISCAL_QTR_NUM,\r\n",
							"            RDT.YEAR_NUM AS FISCAL_YEAR_NUM\r\n",
							"            FROM\r\n",
							"            time.REF_DAY_TIME RDT LEFT JOIN GET_RPT_WEEK_NUM_IN_QTR GT ON \r\n",
							"            RDT.DATE = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,1,RDT.DATE) AS DATE)  = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,2,RDT.DATE) AS DATE) = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,3,RDT.DATE) AS DATE) = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,4,RDT.DATE) AS DATE)= GT.DATE\r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,5,RDT.DATE) AS DATE) = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,6,RDT.DATE) AS DATE) = GT.DATE\r\n",
							"            WHERE\r\n",
							"            RDT.DAY_PERIOD_KEY > 0\r\n",
							"            AND RDT.DATE <= '2022-03-31'\r\n",
							"            -- AND RDT.DATE > '2021-03-31'\r\n",
							"\r\n",
							"            UNION\r\n",
							"\r\n",
							"            SELECT\r\n",
							"            RDT.DATE AS DAY_DATE,\r\n",
							"            RDT.DAY_NUM_IN_WEEK AS DAY_IN_WEEK_NUM,\r\n",
							"            RDT.DAY_NUM_IN_MONTH AS DAY_IN_MONTH_NUM,\r\n",
							"            RDT.DAY_NUM_IN_QTR AS DAY_IN_QTR_NUM,\r\n",
							"            RDT.DAY_OF_WEEK AS DAY_OF_WEEK_NAME,\r\n",
							"            RDT.DAY_OF_WK_SHRT_NM AS DAY_OF_WEEK_SHRT_NAME,\r\n",
							"            RDT.RPT_WEEK_NUM_IN_QTR AS WK_IN_QTR_NUM,\r\n",
							"            GT.RPT_WEEK_NUM_IN_QTR AS RPT_WK_IN_QTR_NUM,\r\n",
							"            WEEKOFYEAR(RDT.DATE) AS WK_IN_YEAR_NUM,\r\n",
							"            RDT.MONTH_NUM_IN_QTR AS MONTH_IN_QTR_NUM,\r\n",
							"            CAST(RDT.MONTH_NUM_IN_YEAR AS SMALLINT) AS MONTH_IN_YEAR_NUM,\r\n",
							"            CASE WHEN CAST(MONTH_NUM_IN_YEAR AS SMALLINT) IN (1,2,3) THEN CAST(MONTH_NUM_IN_YEAR AS SMALLINT) + 9 ELSE CAST(MONTH_NUM_IN_YEAR AS SMALLINT) - 3 END AS MONTH_IN_FISCAL_YEAR_NUM,\r\n",
							"            RDT.MONTH_NAME AS MONTH_NAME,\r\n",
							"            RDT.MONTH_SHRT_NM AS MONTH_SHRT_NAME,\r\n",
							"            CAST(SUBSTR(RDT.QTR,2,1) AS SMALLINT) AS QTR_NUM,\r\n",
							"            RDT.YEAR_NUM AS YEAR_NUM,\r\n",
							"            CAST(CASE WHEN CAST(SUBSTR(TRIM(RDT.QTR),2,1) AS SMALLINT) = 1 THEN 4 ELSE (CAST(SUBSTR(TRIM(RDT.QTR),2,1) AS SMALLINT) -1) END AS SMALLINT) AS FISCAL_QTR_NUM,\r\n",
							"            CAST(CASE WHEN CAST(SUBSTR(TRIM(RDT.QTR),2,1) AS SMALLINT) = 1 THEN CAST(RDT.YEAR AS SMALLINT) ELSE (CAST(RDT.YEAR AS SMALLINT) + 1) END AS SMALLINT) AS FISCAL_YEAR_NUM\r\n",
							"            FROM\r\n",
							"            time.REF_DAY_TIME RDT LEFT JOIN GET_RPT_WEEK_NUM_IN_QTR GT ON \r\n",
							"            RDT.DATE = GT.DATE \r\n",
							"            OR\r\n",
							"            CAST(DATEADD(DAY,1,RDT.DATE) AS DATE)  = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,2,RDT.DATE) AS DATE) = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,3,RDT.DATE) AS DATE) = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,4,RDT.DATE) AS DATE)= GT.DATE\r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,5,RDT.DATE) AS DATE) = GT.DATE \r\n",
							"            OR \r\n",
							"            CAST(DATEADD(DAY,6,RDT.DATE) AS DATE) = GT.DATE\r\n",
							"            WHERE\r\n",
							"            RDT.DAY_PERIOD_KEY > 0\r\n",
							"            AND RDT.DATE > '2022-03-31'         \r\n",
							"            AND RDT.DATE <= '2200-12-31' \r\n",
							"    \"\"\")\r\n",
							"    del_filename = adls_host + basepath + \"REF_DAY_TIME.del\"\r\n",
							"    df.repartition(1).write.option(\"header\",True).option(\"delimiter\",\",\").csv(del_filename,mode='overwrite')\r\n",
							"    logger.info(\"created REF_DAY_TIME.del file \\n\")\r\n",
							"\r\n",
							"if __name__ == \"__main__\":\r\n",
							"    main()\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql import SparkSesion\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"##init table from legacy\r\n",
							"def main():\r\n",
							"\tlogger.info(\"Starting Application ...\")\r\n",
							"\tsparkAppName = \"LoadTimeTables\"\r\n",
							"\t#    spark = SparkSesion.builder.appName(sparkAppName).getOrCreate()\r\n",
							"\tadls_host = token_library.getSecret(key_vault_name,\"ooo-cz-adls-host\",key_vault_linked_service_name)\r\n",
							"\tdel_filename = adls_host + '/KYN_ISIP/time/raw/' + 'REF_DAY_TIME.del'\r\n",
							"\r\n",
							"\t#Reading REF_DAY_TIME.del file into a dataframe\r\n",
							"\tlogger.info(\"Reading REF_DAY_TIME_ESA.csv file..\")\r\n",
							"\tdf = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(del_filename).createOrReplaceTempView(\"REF_DAY_TIME_temp\")\r\n",
							"\r\n",
							"\t#Load TIME.DIM_DAY table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for TIME.DIM_DAY table\")\r\n",
							"\tdf_dim_day = spark.sql(\"\"\"\r\n",
							"\t\tSELECT \r\n",
							"\t\tcast(T.DAY_DATE AS date) as DAY_DATE,\r\n",
							"\t\tcast(((T.YEAR_NUM - 1) * 104) + ((T.QTR_NUM - 1) * 14) + CASE WHEN T.WK_IN_QTR_NUM = 0 THEN 1 ELSE T.WK_IN_QTR_NUM END as integer) AS YEAR_QTR_WK_SEQ,\r\n",
							"\t\tcast(((T.FISCAL_YEAR_NUM - 1) * 104) + ((T.FISCAL_QTR_NUM - 1) * 14) + T.RPT_WK_IN_QTR_NUM as integer) AS YEAR_QTR_WK_SEQ_WED,\r\n",
							"\t\tcast(((T.YEAR_NUM - 1) * 12) + T.MONTH_IN_YEAR_NUM AS integer) as YEAR_MONTH_SEQ,\r\n",
							"\t\tcast(((T.FISCAL_YEAR_NUM - 1) * 12) + T.MONTH_IN_FISCAL_YEAR_NUM AS integer) as  YEAR_MONTH_SEQ_WED,\r\n",
							"\t\tcast(((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS integer) as YEAR_QTR_SEQ,\r\n",
							"\t\tcast(((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM AS integer) as  YEAR_QTR_SEQ_WED,\r\n",
							"\t\tcast(T.DAY_IN_WEEK_NUM as integer) as DAY_IN_WEEK_NUM,\r\n",
							"\t\tcast(T.DAY_IN_MONTH_NUM as integer) as DAY_IN_MONTH_NUM,\r\n",
							"\t\tcast(T.DAY_IN_QTR_NUM as integer) as DAY_IN_QTR_NUM,\r\n",
							"\t\tT.DAY_OF_WEEK_NAME,\r\n",
							"\t\tT.DAY_OF_WEEK_SHRT_NAME, \r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
							"\t\tFROM \r\n",
							"\t\tREF_DAY_TIME_temp T\r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_DAY table\")\r\n",
							"\tdf_dim_day.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"TIME.DIM_DAY\")\r\n",
							"\tlogger.info(\"Load Completed:TIME.DIM_DAY\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\r\n",
							"\t#Load TIME.DIM_MONTH_YEAR table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for TIME.DIM_MONTH_YEAR table\")\r\n",
							"\tdf_DIM_MONTH_YEAR = spark.sql(\"\"\"\r\n",
							"\t\tSELECT DISTINCT\r\n",
							"\t\tcast(((T.YEAR_NUM - 1) * 12) + T.MONTH_IN_YEAR_NUM AS integer) as YEAR_MONTH_SEQ,\r\n",
							"\t\tcast(((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS integer) as YEAR_QTR_SEQ,\r\n",
							"\t\tcast(T.MONTH_IN_QTR_NUM as integer) as MONTH_IN_QTR_NUM,\r\n",
							"\t\tcast(T.MONTH_IN_YEAR_NUM as integer) as MONTH_IN_YEAR_NUM,\r\n",
							"\t\tT.MONTH_NAME,\r\n",
							"\t\tT.MONTH_SHRT_NAME,\r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
							"\t\tFROM \r\n",
							"\t\tREF_DAY_TIME_temp T\r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_MONTH_YEAR table\")\r\n",
							"\tdf_DIM_MONTH_YEAR.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"TIME.DIM_MONTH_YEAR\")\r\n",
							"\tlogger.info(\"Load Completed:DIM_MONTH_YEAR\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\r\n",
							"\r\n",
							"\t#Load TIME.DIM_WK_QTR_YEAR table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for TIME.DIM_WK_QTR_YEAR table\")\r\n",
							"\tdf_DIM_WK_QTR_YEAR = spark.sql(\"\"\"\r\n",
							"\t\twith temp2 AS (\r\n",
							"\t\tSELECT YEAR_QTR_WK_SEQ,\r\n",
							"\t\tYEAR_QTR_SEQ, \r\n",
							"\t\tWK_IN_QTR_NUM,\r\n",
							"\t\tWK_IN_YEAR_NUM, \r\n",
							"\t\tROW_NUMBER() OVER \r\n",
							"\t\t(PARTITION  BY YEAR_QTR_WK_SEQ ORDER BY WK_IN_YEAR_NUM) RN  FROM (\r\n",
							"\t\tSELECT DISTINCT \r\n",
							"\t\t\t((T.YEAR_NUM - 1) * 104) + ((T.QTR_NUM - 1) * 14) + CASE WHEN T.WK_IN_QTR_NUM = 0 THEN 1 ELSE T.WK_IN_QTR_NUM  END AS YEAR_QTR_WK_SEQ,\r\n",
							"\t\t\t((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS YEAR_QTR_SEQ,\r\n",
							"\t\t\tT.WK_IN_QTR_NUM,\r\n",
							"\t\t\tT.WK_IN_YEAR_NUM\r\n",
							"\t\tFROM \r\n",
							"\t\tREF_DAY_TIME_temp T) temp1\r\n",
							"\t\t)\r\n",
							"\t\tSELECT \r\n",
							"\t\tcast(YEAR_QTR_WK_SEQ as integer) as YEAR_QTR_WK_SEQ,\r\n",
							"\t\tcast(YEAR_QTR_SEQ as integer) as YEAR_QTR_SEQ, \r\n",
							"\t\tcast(WK_IN_QTR_NUM as integer) as WK_IN_QTR_NUM, \r\n",
							"\t\tcast(WK_IN_YEAR_NUM as integer) as WK_IN_YEAR_NUM, \r\n",
							"\t\tcurrent_timestamp AS IW_ROW_UPDT_TS\r\n",
							"\t\tFROM temp2 WHERE RN = 1 \r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_WK_QTR_YEAR table\")\r\n",
							"\tdf_DIM_WK_QTR_YEAR.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"TIME.DIM_WK_QTR_YEAR\")\r\n",
							"\tlogger.info(\"Load Completed: TIME.DIM_WK_QTR_YEAR\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\r\n",
							"\t#Load TIME.DIM_QTR_YEAR table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for TIME.DIM_QTR_YEAR table\")\r\n",
							"\tdf_DIM_QTR_YEAR = spark.sql(\"\"\"\r\n",
							"\t\tSELECT DISTINCT\r\n",
							"\t\tcast((((T.YEAR_NUM - 1) * 4) + T.QTR_NUM) as Integer) AS YEAR_QTR_SEQ,\r\n",
							"\t\tcast(T.QTR_NUM as Integer) as QTR_NUM,\r\n",
							"\t\tcast(T.YEAR_NUM as Integer) as YEAR_NUM,\r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
							"\t\tFROM \r\n",
							"\t\tREF_DAY_TIME_temp T\r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_QTR_YEAR table\")\r\n",
							"\tdf_DIM_QTR_YEAR.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_YEAR\")\r\n",
							"\tlogger.info(\"Load Completed: time.DIM_QTR_YEAR\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\r\n",
							"\t#Load TIME.DIM_WK_QTR_YEAR_BUS table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for TIME.DIM_WK_QTR_YEAR_BUS table\")\r\n",
							"\tdf_DIM_WK_QTR_YEAR_BUS = spark.sql(\"\"\"\r\n",
							"\t\twith MAX_RPT_WEEK_IN_QTR AS (\r\n",
							"\t\tSELECT YEAR_NUM, QTR_NUM, COUNT(DISTINCT RPT_WK_IN_QTR_NUM) MAX_RPT_WEEK_IN_QTR_NUM FROM  REF_DAY_TIME_temp WHERE\r\n",
							"\t\tDAY_DATE BETWEEN '1900-01-01' AND '2200-12-31' GROUP BY YEAR_NUM, QTR_NUM\r\n",
							"\t\t)\r\n",
							"\t\tSELECT DISTINCT \r\n",
							"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 104) + ((T.FISCAL_QTR_NUM - 1) * 14) + T.RPT_WK_IN_QTR_NUM) as integer) AS YEAR_QTR_WK_SEQ_WED,\r\n",
							"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
							"\t\tcast(T.RPT_WK_IN_QTR_NUM as Integer) as WK_IN_QTR_NUM_WED,\r\n",
							"\t\tcast((T.RPT_WK_IN_QTR_NUM - (MRW.MAX_RPT_WEEK_IN_QTR_NUM + 1)) as integer) AS WK_RO_IN_QTR_NUM_WED,\r\n",
							"\t\tCAST(CASE WHEN T.RPT_WK_IN_QTR_NUM = MRW.MAX_RPT_WEEK_IN_QTR_NUM THEN 'Y' ELSE 'N' END  AS CHAR(3)) AS FV_WEEKS_INDC,\r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
							"\t\tFROM \r\n",
							"\t\tREF_DAY_TIME_temp T\r\n",
							"\t\tLEFT JOIN MAX_RPT_WEEK_IN_QTR MRW ON T.YEAR_NUM = MRW.YEAR_NUM AND T.QTR_NUM = MRW.QTR_NUM\r\n",
							"\t\tORDER BY 1\r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_WK_QTR_YEAR_BUS table\")\r\n",
							"\tdf_DIM_WK_QTR_YEAR_BUS.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_WK_QTR_YEAR_BUS\")\r\n",
							"\tlogger.info(\"Load Completed: time.DIM_WK_QTR_YEAR_BUS\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\r\n",
							"\r\n",
							"\t#Load TIME.DIM_MONTH_YEAR_BUS table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for TIME.DIM_MONTH_YEAR_BUS table\")\r\n",
							"\tdf_DIM_MONTH_YEAR_BUS = spark.sql(\"\"\"\r\n",
							"\t\tSELECT DISTINCT \r\n",
							"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 12) + T.MONTH_IN_FISCAL_YEAR_NUM) as integer) AS YEAR_MONTH_SEQ_WED,\r\n",
							"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
							"\t\tcast(T.MONTH_IN_FISCAL_YEAR_NUM as Integer) as MONTH_IN_FISCAL_YEAR_NUM,\r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
							"\t\tFROM \r\n",
							"\t\tREF_DAY_TIME_temp T\r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_MONTH_YEAR_BUS table\")\r\n",
							"\tdf_DIM_MONTH_YEAR_BUS.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_MONTH_YEAR_BUS\")\r\n",
							"\tlogger.info(\"Load Completed:  Time.DIM_MONTH_YEAR_BUS\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\t#Load TIME.DIM_QTR_YEAR_BUS table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for table time.DIM_QTR_YEAR_BUS\")\r\n",
							"\tdf_DIM_QTR_YEAR_BUS = spark.sql(\"\"\"\r\n",
							"\t\tSELECT DISTINCT \r\n",
							"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
							"\t\tcast(T.FISCAL_QTR_NUM as integer) as FISCAL_QTR_NUM,\r\n",
							"\t\tcast(T.FISCAL_YEAR_NUM as integer) as FISCAL_YEAR_NUM,\r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
							"\t\tFROM \r\n",
							"\t\tREF_DAY_TIME_temp T\r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_QTR_YEAR_BUS table\")\r\n",
							"\tdf_DIM_QTR_YEAR_BUS.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_YEAR_BUS\")\r\n",
							"\tlogger.info(\"Load Completed: time.DIM_QTR_YEAR_BUS\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\r\n",
							"\t#Load TIME.DIM_QTR_MNEMONIC_OFFSET table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for table time.DIM_QTR_MNEMONIC_OFFSET from DIM_QTR_MNEMONIC_OFFSET.csv\")\r\n",
							"\tmnemonic_file = adls_host + '/KYN_ISIP/time/raw/' + \"DIM_QTR_MNEMONIC_OFFSET.csv\"\r\n",
							"\tlogger.info(\"Reaing DIM_QTR_MNEMONIC_OFFSET.csv file\")\r\n",
							"\tdf_mnemonic_file = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(mnemonic_file).createOrReplaceTempView(\"DIM_QTR_MNEMONIC_OFFSET_TEMP\")\r\n",
							"\tdf_DIM_QTR_MNEMONIC_OFFSET = spark.sql(\"\"\"\r\n",
							"\t\tselect distinct\r\n",
							"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
							"\t\tcast(((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS integer) as YEAR_QTR_SEQ,\r\n",
							"\t\tcast(((T.YEAR_NUM) * 4 + T.QTR_NUM) - (YEAR(CURRENT_DATE) * 4 + QUARTER(CURRENT_DATE)) AS SMALLINT) AS DAILY_QTR_MNEMONIC_OFFSET,\r\n",
							"\t\tcast(((T.YEAR_NUM) * 4 + T.QTR_NUM) - (YEAR(CURRENT_DATE) * 4 + QUARTER(CURRENT_DATE)) AS SMALLINT) AS QTR_MNEMONIC_OFFSET,\r\n",
							"\t\tCASE \r\n",
							"\t\tWHEN YEAR(CURRENT_DATE) = T.YEAR_NUM AND QUARTER(CURRENT_DATE) = T.QTR_NUM THEN 'Y' \r\n",
							"\t\tELSE 'N' \r\n",
							"\t\tEND AS CURR_QTR_INDC,\r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS from REF_DAY_TIME_temp T\r\n",
							"\t\"\"\")\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_QTR_MNEMONIC_OFFSET table\")\r\n",
							"\tdf_DIM_QTR_MNEMONIC_OFFSET.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_MNEMONIC_OFFSET\")\r\n",
							"\tlogger.info(\"Load Completed: time.DIM_QTR_MNEMONIC_OFFSET\")\r\n",
							"\tlogger.info(\"--------------------------------------------------\")\r\n",
							"\r\n",
							"\t#Load TIME.DIM_MNEMONIC table.\r\n",
							"\tlogger.info(\"Executing SQL to get data for table time.DIM_MNEMONIC from DIM_MNEMONIC.csv\")\r\n",
							"\tmnemonic_file = adls_host + '/KYN_ISIP/time/raw/' + \"DIM_MNEMONIC.csv\"\r\n",
							"\tlogger.info(\"Reading DIM_MNEMONIC.csv file\")\r\n",
							"\tdf_mnemonic_file = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(mnemonic_file).createOrReplaceTempView(\"DIM_MNEMONIC_TEMP\")\r\n",
							"\r\n",
							"\tdf_DIM_MNEMONIC = spark.sql(\"\"\"\r\n",
							"\t\tselect \r\n",
							"\t\tcast(T.DIM_QTR_MNEMONIC_OFFSET as integer) as DIM_QTR_MNEMONIC_OFFSET,\r\n",
							"\t\tQTR_MNEMONIC_ID as QTR_MNEMONIC_ID,\r\n",
							"\t\tQTR_MNEMONIC_DESCR as QTR_MNEMONIC_DESCR,\r\n",
							"\t\tACTIVE_INDC as ACTIVE_INDC,\r\n",
							"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS from DIM_MNEMONIC_TEMP T\r\n",
							"\t\"\"\")\r\n",
							"\r\n",
							"\tlogger.info(\"Writing data into TIME.DIM_MNEMONIC table\")\r\n",
							"\tdf_DIM_MNEMONIC.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_MNEMONIC\")\r\n",
							"\tlogger.info(\"Load Completed: time.DIM_MNEMONIC\")\r\n",
							"\tlogger.info(\"------------------END OF TIME DIMENSION TABLE LOADS---------------\")\r\n",
							"\r\n",
							"if __name__ == \"__main__\":\r\n",
							"    main()\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Time_Setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Time/install"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Setup Notebook for Time Ingestion\r\n",
							"# creates schema and tables for Time ingestion\r\n",
							"# Upload REF_DAY_TIME_ESA.csv, DIM_QTR_MNEMONIC_OFFSET.csv static files into KYN_ISIP/time/raw folder\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/utils-adls-credentials"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE SCHEMA Time LOCATION '{0}/KYN_ISIP/time/lakedb'\".format(adls_host))"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testNotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.notebook.exit(\"Done!!!\") "
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse_db')]",
			"type": "Microsoft.Synapse/workspaces/databases",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"Ddls": [
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "synapse_db",
							"EntityType": "DATABASE",
							"Origin": {
								"Type": "SPARK"
							},
							"Properties": {
								"IsSyMSCDMDatabase": true
							},
							"Source": {
								"Provider": "ADLS",
								"Location": "abfss://dev-synapse-fs@devsynapsedl2.dfs.core.windows.net/synapse_db",
								"Properties": {
									"FormatType": "parquet",
									"LinkedServiceName": "DataLake_DEV_LS"
								}
							}
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "synapse_dev",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "synapse_db"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "Id",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "Name",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": false,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "RG",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "ApplicationId",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "ProjectName",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "Application_Management_Group",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "DataRetention",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "DeployDate",
										"OriginDataTypeName": {
											"TypeName": "timestamp",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"TimestampFormat": "YYYY-MM-DD HH:MM:SS.fffffffff",
												"HIVE_TYPE_STRING": "timestamp"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "Support_Group",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "Environment",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "ProjectOwner",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "TicketNo",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "VM",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "DeployBy",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat",
									"FormatType": "parquet",
									"SerializeLib": "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe",
									"Properties": {
										"path": "abfss://dev-synapse-fs@devsynapsedl2.dfs.core.windows.net/parquet/synapses.parquet",
										"FormatTypeSetToDatabaseDefault": false
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://dev-synapse-fs@devsynapsedl2.dfs.core.windows.net/parquet/synapses.parquet",
									"Properties": {
										"LinkedServiceName": "DataLake_DEV_LS",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "",
								"spark.sql.sources.provider": "parquet"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "synapse_dev_csv",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "synapse_db"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "C1",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "C2",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "C3",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "C4",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "C5",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										},
										"DeltaFormatInvalidMessages": []
									},
									{
										"Name": "C6",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"DeltaFormatInvalidMessages": []
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://dev-synapse-fs@devsynapsedl2.dfs.core.windows.net/synapse_git.csv",
										"delimiter": ",",
										"multiLine": "false",
										"firstRowAsHeader": "false",
										"serialization.format": "1",
										"FormatTypeSetToDatabaseDefault": false,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://dev-synapse-fs@devsynapsedl2.dfs.core.windows.net/synapse_git.csv",
									"Properties": {
										"LinkedServiceName": "DataLake_DEV_LS",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/DataLake_DEV_PE')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/ee04e0d4-c40f-44eb-b085-5c1a090f2e87/resourceGroups/Delme/providers/Microsoft.Storage/storageAccounts/devsynapsedl2",
				"groupId": "dfs"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sql--dev-synapse-workspace-1973')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/ee04e0d4-c40f-44eb-b085-5c1a090f2e87/resourceGroups/Delme/providers/Microsoft.Synapse/workspaces/dev-synapse-workspace-1973",
				"groupId": "sql",
				"fqdns": [
					"dev-synapse-workspace-1973.2e0a0cac-3305-4d00-8be1-3f38846d58ff.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sqlOnDemand--dev-synapse-workspace-1973')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/ee04e0d4-c40f-44eb-b085-5c1a090f2e87/resourceGroups/Delme/providers/Microsoft.Synapse/workspaces/dev-synapse-workspace-1973",
				"groupId": "sqlOnDemand",
				"fqdns": [
					"dev-synapse-workspace-1973-ondemand.2e0a0cac-3305-4d00-8be1-3f38846d58ff.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		}
	]
}