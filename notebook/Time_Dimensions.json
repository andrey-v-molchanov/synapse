{
	"name": "Time_Dimensions",
	"properties": {
		"folder": {
			"name": "Time"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "synspisips001",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "24e84a26-59bb-41c5-be5c-2fd398ba2697"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/0482d4e6-19e6-4955-8af3-3eb7a14379ac/resourceGroups/rg-cdo-ooo-dev-weu-001/providers/Microsoft.Synapse/workspaces/syn-ooo-isip-dev-weu-001/bigDataPools/synspisips001",
				"name": "synspisips001",
				"type": "Spark",
				"endpoint": "https://syn-ooo-isip-dev-weu-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/synspisips001",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#This notebook is to create REF_DAY_TIME.del file from REF_DAY_TIME_ESA.csv and load the del file into time.REF_DAY_TIME table.\r\n",
					"#Use time.REF_DAY_TIME table to populate other time dimension tables."
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/utils-adls-credentials"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/utils-setup-logger"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\r\n",
					"\r\n",
					"##init table from legacy\r\n",
					"def main():\r\n",
					"    logger.info(\"Starting Application ...\")\r\n",
					"    sparkAppName = \"CreateREF_DAY_TIMEdel\"\r\n",
					"    spark = SparkSession.builder.appName(sparkAppName).getOrCreate()\r\n",
					"    adls_host = token_library.getSecret(key_vault_name,\"ooo-cz-adls-host\",key_vault_linked_service_name)\r\n",
					"    basepath = \"/KYN_ISIP/time/raw/\"\r\n",
					"\r\n",
					"    # Reading REF_DAY_TIME_ESA.csv file and loading data into time.REF_DAY_TIME table\r\n",
					"    logger.info(\"Started to create REF_DAY_TIME.del file from  REF_DAY_TIME_ESA.csv : \\n\")\r\n",
					"    customSchema = StructType([ \r\n",
					"        StructField(\"DAY_PERIOD_KEY\",  IntegerType(), True),\r\n",
					"        StructField(\"DATE\", DateType(), True),\r\n",
					"        StructField(\"DAY_NUM_IN_WEEK\",  IntegerType(), True),\r\n",
					"        StructField(\"DAY_NUM_IN_MONTH\",  IntegerType(), True),\r\n",
					"        StructField(\"DAY_NUM_IN_QTR\",  IntegerType(), True),\r\n",
					"        StructField(\"DAY_NUM_IN_EPOCH\",  IntegerType(), True),\r\n",
					"        StructField(\"RPT_DAY_NUM_IN_QTR\",  IntegerType(), True),\r\n",
					"        StructField(\"DAY_OF_WEEK\", StringType(), True),\r\n",
					"        StructField(\"DAY_OF_WK_SHRT_NM\", StringType(), True),\r\n",
					"        StructField(\"RPT_WEEK_NUM_IN_MONTH\",  IntegerType(), True),\r\n",
					"        StructField(\"RPT_WEEK_NUM_IN_QTR\",  IntegerType(), True),\r\n",
					"        StructField(\"WEEK_NUM_IN_YEAR\",  IntegerType(), True),\r\n",
					"        StructField(\"WEEK_NUM_IN_EPOCH\",  IntegerType(), True),\r\n",
					"        StructField(\"MONTH_NUM_IN_QTR\",  IntegerType(), True),\r\n",
					"        StructField(\"MONTH_NUM_IN_YEAR\", StringType(), True),\r\n",
					"        StructField(\"MONTH_NUM_IN_EPOCH\",  IntegerType(), True),\r\n",
					"        StructField(\"MONTH_NAME\", StringType(), True),\r\n",
					"        StructField(\"MONTH_SHRT_NM\", StringType(), True),\r\n",
					"        StructField(\"QTR\", StringType(), True),\r\n",
					"        StructField(\"YEAR\", StringType(), True),\r\n",
					"        StructField(\"LOAD_RPT_QTR\", StringType(), True),\r\n",
					"        StructField(\"LOAD_RPT_YEAR\", StringType(), True),\r\n",
					"        StructField(\"WEEK_NUM_IN_QTR\",  IntegerType(), True),\r\n",
					"        StructField(\"YEAR_NUM\",  IntegerType(), True),\r\n",
					"        StructField(\"FIFTEENWK_WK_IN_QTR_NUM\",  IntegerType(), True),\r\n",
					"        StructField(\"FIFTEENWK_QTR_NUM\",  IntegerType(), True),\r\n",
					"        StructField(\"FIFTEENWK_QTR_CODE\", StringType(), True),\r\n",
					"        StructField(\"FIFTEENWK_YEAR_NUM\",  IntegerType(), True),\r\n",
					"        StructField(\"FIFTEENWK_YEAR\",  StringType(), True)\r\n",
					"    ])\r\n",
					"    logger.info(\"Reading REF_DAY_TIME_ESA.csv file..\")\r\n",
					"    ref_filename = adls_host + basepath + \"REF_DAY_TIME_ESA.csv\"\r\n",
					"    df = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(ref_filename)\r\n",
					"    df.write.mode(\"overwrite\").saveAsTable(\"time.REF_DAY_TIME\")\r\n",
					"    logger.info(\"Saved data into time.REF_DAY_TIME table\")\r\n",
					"\r\n",
					"    #Using time.REF_DAY_TIME to create REF_DAY_TIME.del for REF_DAY_TIME.del\r\n",
					"    logger.info(\"Started SQL logic to populate data for  REF_DAY_TIME.del file\")\r\n",
					"    df = spark.sql(\"\"\"\r\n",
					"            WITH GET_RPT_WEEK_NUM_IN_QTR AS (\r\n",
					"            SELECT AL1.DATE,\r\n",
					"            CASE WHEN AL2.MIN_DAY_NUM_IN_WEEK IN (4,5,6,7) THEN AL1.RPT_WEEK_NUM_IN_QTR - 1 ELSE AL1.RPT_WEEK_NUM_IN_QTR END AS RPT_WEEK_NUM_IN_QTR\r\n",
					"            FROM time.REF_DAY_TIME AL1\r\n",
					"            LEFT JOIN (\r\n",
					"            SELECT YEAR_NUM, QTR, MIN(DAY_NUM_IN_WEEK) MIN_DAY_NUM_IN_WEEK FROM \r\n",
					"            time.REF_DAY_TIME WHERE RPT_WEEK_NUM_IN_QTR = 1 AND MONTH_NUM_IN_QTR = 1 AND \r\n",
					"            DATE BETWEEN '1900-01-01' AND '2200-12-31' GROUP BY YEAR_NUM, QTR)  AL2 \r\n",
					"            ON AL1.YEAR_NUM = AL2.YEAR_NUM AND AL1.QTR = AL2.QTR\r\n",
					"            WHERE DAY_NUM_IN_WEEK = 3 AND DATE BETWEEN '1900-01-01' AND '2200-12-31' )\r\n",
					"            SELECT\r\n",
					"            RDT.DATE AS DAY_DATE,\r\n",
					"            RDT.DAY_NUM_IN_WEEK AS DAY_IN_WEEK_NUM,\r\n",
					"            RDT.DAY_NUM_IN_MONTH AS DAY_IN_MONTH_NUM,\r\n",
					"            RDT.DAY_NUM_IN_QTR AS DAY_IN_QTR_NUM,\r\n",
					"            RDT.DAY_OF_WEEK AS DAY_OF_WEEK_NAME,\r\n",
					"            RDT.DAY_OF_WK_SHRT_NM AS DAY_OF_WEEK_SHRT_NAME,\r\n",
					"            RDT.RPT_WEEK_NUM_IN_QTR AS WK_IN_QTR_NUM,\r\n",
					"            GT.RPT_WEEK_NUM_IN_QTR AS RPT_WK_IN_QTR_NUM,\r\n",
					"            WEEKOFYEAR(RDT.DATE) AS WK_IN_YEAR_NUM,\r\n",
					"            RDT.MONTH_NUM_IN_QTR AS MONTH_IN_QTR_NUM,\r\n",
					"            CAST(RDT.MONTH_NUM_IN_YEAR AS SMALLINT) AS MONTH_IN_YEAR_NUM,\r\n",
					"            CAST(RDT.MONTH_NUM_IN_YEAR AS SMALLINT) AS MONTH_IN_FISCAL_YEAR_NUM,\r\n",
					"            RDT.MONTH_NAME AS MONTH_NAME,\r\n",
					"            RDT.MONTH_SHRT_NM AS MONTH_SHRT_NAME,\r\n",
					"            CAST(SUBSTR(RDT.QTR,2,1) AS SMALLINT) AS QTR_NUM,\r\n",
					"            RDT.YEAR_NUM AS YEAR_NUM,\r\n",
					"            CAST(SUBSTR(RDT.QTR,2,1) AS SMALLINT) AS FISCAL_QTR_NUM,\r\n",
					"            RDT.YEAR_NUM AS FISCAL_YEAR_NUM\r\n",
					"            FROM\r\n",
					"            time.REF_DAY_TIME RDT LEFT JOIN GET_RPT_WEEK_NUM_IN_QTR GT ON \r\n",
					"            RDT.DATE = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,1,RDT.DATE) AS DATE)  = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,2,RDT.DATE) AS DATE) = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,3,RDT.DATE) AS DATE) = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,4,RDT.DATE) AS DATE)= GT.DATE\r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,5,RDT.DATE) AS DATE) = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,6,RDT.DATE) AS DATE) = GT.DATE\r\n",
					"            WHERE\r\n",
					"            RDT.DAY_PERIOD_KEY > 0\r\n",
					"            AND RDT.DATE <= '2022-03-31'\r\n",
					"            -- AND RDT.DATE > '2021-03-31'\r\n",
					"\r\n",
					"            UNION\r\n",
					"\r\n",
					"            SELECT\r\n",
					"            RDT.DATE AS DAY_DATE,\r\n",
					"            RDT.DAY_NUM_IN_WEEK AS DAY_IN_WEEK_NUM,\r\n",
					"            RDT.DAY_NUM_IN_MONTH AS DAY_IN_MONTH_NUM,\r\n",
					"            RDT.DAY_NUM_IN_QTR AS DAY_IN_QTR_NUM,\r\n",
					"            RDT.DAY_OF_WEEK AS DAY_OF_WEEK_NAME,\r\n",
					"            RDT.DAY_OF_WK_SHRT_NM AS DAY_OF_WEEK_SHRT_NAME,\r\n",
					"            RDT.RPT_WEEK_NUM_IN_QTR AS WK_IN_QTR_NUM,\r\n",
					"            GT.RPT_WEEK_NUM_IN_QTR AS RPT_WK_IN_QTR_NUM,\r\n",
					"            WEEKOFYEAR(RDT.DATE) AS WK_IN_YEAR_NUM,\r\n",
					"            RDT.MONTH_NUM_IN_QTR AS MONTH_IN_QTR_NUM,\r\n",
					"            CAST(RDT.MONTH_NUM_IN_YEAR AS SMALLINT) AS MONTH_IN_YEAR_NUM,\r\n",
					"            CASE WHEN CAST(MONTH_NUM_IN_YEAR AS SMALLINT) IN (1,2,3) THEN CAST(MONTH_NUM_IN_YEAR AS SMALLINT) + 9 ELSE CAST(MONTH_NUM_IN_YEAR AS SMALLINT) - 3 END AS MONTH_IN_FISCAL_YEAR_NUM,\r\n",
					"            RDT.MONTH_NAME AS MONTH_NAME,\r\n",
					"            RDT.MONTH_SHRT_NM AS MONTH_SHRT_NAME,\r\n",
					"            CAST(SUBSTR(RDT.QTR,2,1) AS SMALLINT) AS QTR_NUM,\r\n",
					"            RDT.YEAR_NUM AS YEAR_NUM,\r\n",
					"            CAST(CASE WHEN CAST(SUBSTR(TRIM(RDT.QTR),2,1) AS SMALLINT) = 1 THEN 4 ELSE (CAST(SUBSTR(TRIM(RDT.QTR),2,1) AS SMALLINT) -1) END AS SMALLINT) AS FISCAL_QTR_NUM,\r\n",
					"            CAST(CASE WHEN CAST(SUBSTR(TRIM(RDT.QTR),2,1) AS SMALLINT) = 1 THEN CAST(RDT.YEAR AS SMALLINT) ELSE (CAST(RDT.YEAR AS SMALLINT) + 1) END AS SMALLINT) AS FISCAL_YEAR_NUM\r\n",
					"            FROM\r\n",
					"            time.REF_DAY_TIME RDT LEFT JOIN GET_RPT_WEEK_NUM_IN_QTR GT ON \r\n",
					"            RDT.DATE = GT.DATE \r\n",
					"            OR\r\n",
					"            CAST(DATEADD(DAY,1,RDT.DATE) AS DATE)  = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,2,RDT.DATE) AS DATE) = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,3,RDT.DATE) AS DATE) = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,4,RDT.DATE) AS DATE)= GT.DATE\r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,5,RDT.DATE) AS DATE) = GT.DATE \r\n",
					"            OR \r\n",
					"            CAST(DATEADD(DAY,6,RDT.DATE) AS DATE) = GT.DATE\r\n",
					"            WHERE\r\n",
					"            RDT.DAY_PERIOD_KEY > 0\r\n",
					"            AND RDT.DATE > '2022-03-31'         \r\n",
					"            AND RDT.DATE <= '2200-12-31' \r\n",
					"    \"\"\")\r\n",
					"    del_filename = adls_host + basepath + \"REF_DAY_TIME.del\"\r\n",
					"    df.repartition(1).write.option(\"header\",True).option(\"delimiter\",\",\").csv(del_filename,mode='overwrite')\r\n",
					"    logger.info(\"created REF_DAY_TIME.del file \\n\")\r\n",
					"\r\n",
					"if __name__ == \"__main__\":\r\n",
					"    main()\r\n",
					"\r\n",
					""
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql import SparkSesion\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"##init table from legacy\r\n",
					"def main():\r\n",
					"\tlogger.info(\"Starting Application ...\")\r\n",
					"\tsparkAppName = \"LoadTimeTables\"\r\n",
					"\t#    spark = SparkSesion.builder.appName(sparkAppName).getOrCreate()\r\n",
					"\tadls_host = token_library.getSecret(key_vault_name,\"ooo-cz-adls-host\",key_vault_linked_service_name)\r\n",
					"\tdel_filename = adls_host + '/KYN_ISIP/time/raw/' + 'REF_DAY_TIME.del'\r\n",
					"\r\n",
					"\t#Reading REF_DAY_TIME.del file into a dataframe\r\n",
					"\tlogger.info(\"Reading REF_DAY_TIME_ESA.csv file..\")\r\n",
					"\tdf = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(del_filename).createOrReplaceTempView(\"REF_DAY_TIME_temp\")\r\n",
					"\r\n",
					"\t#Load TIME.DIM_DAY table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for TIME.DIM_DAY table\")\r\n",
					"\tdf_dim_day = spark.sql(\"\"\"\r\n",
					"\t\tSELECT \r\n",
					"\t\tcast(T.DAY_DATE AS date) as DAY_DATE,\r\n",
					"\t\tcast(((T.YEAR_NUM - 1) * 104) + ((T.QTR_NUM - 1) * 14) + CASE WHEN T.WK_IN_QTR_NUM = 0 THEN 1 ELSE T.WK_IN_QTR_NUM END as integer) AS YEAR_QTR_WK_SEQ,\r\n",
					"\t\tcast(((T.FISCAL_YEAR_NUM - 1) * 104) + ((T.FISCAL_QTR_NUM - 1) * 14) + T.RPT_WK_IN_QTR_NUM as integer) AS YEAR_QTR_WK_SEQ_WED,\r\n",
					"\t\tcast(((T.YEAR_NUM - 1) * 12) + T.MONTH_IN_YEAR_NUM AS integer) as YEAR_MONTH_SEQ,\r\n",
					"\t\tcast(((T.FISCAL_YEAR_NUM - 1) * 12) + T.MONTH_IN_FISCAL_YEAR_NUM AS integer) as  YEAR_MONTH_SEQ_WED,\r\n",
					"\t\tcast(((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS integer) as YEAR_QTR_SEQ,\r\n",
					"\t\tcast(((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM AS integer) as  YEAR_QTR_SEQ_WED,\r\n",
					"\t\tcast(T.DAY_IN_WEEK_NUM as integer) as DAY_IN_WEEK_NUM,\r\n",
					"\t\tcast(T.DAY_IN_MONTH_NUM as integer) as DAY_IN_MONTH_NUM,\r\n",
					"\t\tcast(T.DAY_IN_QTR_NUM as integer) as DAY_IN_QTR_NUM,\r\n",
					"\t\tT.DAY_OF_WEEK_NAME,\r\n",
					"\t\tT.DAY_OF_WEEK_SHRT_NAME, \r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
					"\t\tFROM \r\n",
					"\t\tREF_DAY_TIME_temp T\r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_DAY table\")\r\n",
					"\tdf_dim_day.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"TIME.DIM_DAY\")\r\n",
					"\tlogger.info(\"Load Completed:TIME.DIM_DAY\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\r\n",
					"\t#Load TIME.DIM_MONTH_YEAR table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for TIME.DIM_MONTH_YEAR table\")\r\n",
					"\tdf_DIM_MONTH_YEAR = spark.sql(\"\"\"\r\n",
					"\t\tSELECT DISTINCT\r\n",
					"\t\tcast(((T.YEAR_NUM - 1) * 12) + T.MONTH_IN_YEAR_NUM AS integer) as YEAR_MONTH_SEQ,\r\n",
					"\t\tcast(((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS integer) as YEAR_QTR_SEQ,\r\n",
					"\t\tcast(T.MONTH_IN_QTR_NUM as integer) as MONTH_IN_QTR_NUM,\r\n",
					"\t\tcast(T.MONTH_IN_YEAR_NUM as integer) as MONTH_IN_YEAR_NUM,\r\n",
					"\t\tT.MONTH_NAME,\r\n",
					"\t\tT.MONTH_SHRT_NAME,\r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
					"\t\tFROM \r\n",
					"\t\tREF_DAY_TIME_temp T\r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_MONTH_YEAR table\")\r\n",
					"\tdf_DIM_MONTH_YEAR.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"TIME.DIM_MONTH_YEAR\")\r\n",
					"\tlogger.info(\"Load Completed:DIM_MONTH_YEAR\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\r\n",
					"\r\n",
					"\t#Load TIME.DIM_WK_QTR_YEAR table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for TIME.DIM_WK_QTR_YEAR table\")\r\n",
					"\tdf_DIM_WK_QTR_YEAR = spark.sql(\"\"\"\r\n",
					"\t\twith temp2 AS (\r\n",
					"\t\tSELECT YEAR_QTR_WK_SEQ,\r\n",
					"\t\tYEAR_QTR_SEQ, \r\n",
					"\t\tWK_IN_QTR_NUM,\r\n",
					"\t\tWK_IN_YEAR_NUM, \r\n",
					"\t\tROW_NUMBER() OVER \r\n",
					"\t\t(PARTITION  BY YEAR_QTR_WK_SEQ ORDER BY WK_IN_YEAR_NUM) RN  FROM (\r\n",
					"\t\tSELECT DISTINCT \r\n",
					"\t\t\t((T.YEAR_NUM - 1) * 104) + ((T.QTR_NUM - 1) * 14) + CASE WHEN T.WK_IN_QTR_NUM = 0 THEN 1 ELSE T.WK_IN_QTR_NUM  END AS YEAR_QTR_WK_SEQ,\r\n",
					"\t\t\t((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS YEAR_QTR_SEQ,\r\n",
					"\t\t\tT.WK_IN_QTR_NUM,\r\n",
					"\t\t\tT.WK_IN_YEAR_NUM\r\n",
					"\t\tFROM \r\n",
					"\t\tREF_DAY_TIME_temp T) temp1\r\n",
					"\t\t)\r\n",
					"\t\tSELECT \r\n",
					"\t\tcast(YEAR_QTR_WK_SEQ as integer) as YEAR_QTR_WK_SEQ,\r\n",
					"\t\tcast(YEAR_QTR_SEQ as integer) as YEAR_QTR_SEQ, \r\n",
					"\t\tcast(WK_IN_QTR_NUM as integer) as WK_IN_QTR_NUM, \r\n",
					"\t\tcast(WK_IN_YEAR_NUM as integer) as WK_IN_YEAR_NUM, \r\n",
					"\t\tcurrent_timestamp AS IW_ROW_UPDT_TS\r\n",
					"\t\tFROM temp2 WHERE RN = 1 \r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_WK_QTR_YEAR table\")\r\n",
					"\tdf_DIM_WK_QTR_YEAR.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"TIME.DIM_WK_QTR_YEAR\")\r\n",
					"\tlogger.info(\"Load Completed: TIME.DIM_WK_QTR_YEAR\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\r\n",
					"\t#Load TIME.DIM_QTR_YEAR table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for TIME.DIM_QTR_YEAR table\")\r\n",
					"\tdf_DIM_QTR_YEAR = spark.sql(\"\"\"\r\n",
					"\t\tSELECT DISTINCT\r\n",
					"\t\tcast((((T.YEAR_NUM - 1) * 4) + T.QTR_NUM) as Integer) AS YEAR_QTR_SEQ,\r\n",
					"\t\tcast(T.QTR_NUM as Integer) as QTR_NUM,\r\n",
					"\t\tcast(T.YEAR_NUM as Integer) as YEAR_NUM,\r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
					"\t\tFROM \r\n",
					"\t\tREF_DAY_TIME_temp T\r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_QTR_YEAR table\")\r\n",
					"\tdf_DIM_QTR_YEAR.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_YEAR\")\r\n",
					"\tlogger.info(\"Load Completed: time.DIM_QTR_YEAR\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\r\n",
					"\t#Load TIME.DIM_WK_QTR_YEAR_BUS table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for TIME.DIM_WK_QTR_YEAR_BUS table\")\r\n",
					"\tdf_DIM_WK_QTR_YEAR_BUS = spark.sql(\"\"\"\r\n",
					"\t\twith MAX_RPT_WEEK_IN_QTR AS (\r\n",
					"\t\tSELECT YEAR_NUM, QTR_NUM, COUNT(DISTINCT RPT_WK_IN_QTR_NUM) MAX_RPT_WEEK_IN_QTR_NUM FROM  REF_DAY_TIME_temp WHERE\r\n",
					"\t\tDAY_DATE BETWEEN '1900-01-01' AND '2200-12-31' GROUP BY YEAR_NUM, QTR_NUM\r\n",
					"\t\t)\r\n",
					"\t\tSELECT DISTINCT \r\n",
					"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 104) + ((T.FISCAL_QTR_NUM - 1) * 14) + T.RPT_WK_IN_QTR_NUM) as integer) AS YEAR_QTR_WK_SEQ_WED,\r\n",
					"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
					"\t\tcast(T.RPT_WK_IN_QTR_NUM as Integer) as WK_IN_QTR_NUM_WED,\r\n",
					"\t\tcast((T.RPT_WK_IN_QTR_NUM - (MRW.MAX_RPT_WEEK_IN_QTR_NUM + 1)) as integer) AS WK_RO_IN_QTR_NUM_WED,\r\n",
					"\t\tCAST(CASE WHEN T.RPT_WK_IN_QTR_NUM = MRW.MAX_RPT_WEEK_IN_QTR_NUM THEN 'Y' ELSE 'N' END  AS CHAR(3)) AS FV_WEEKS_INDC,\r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
					"\t\tFROM \r\n",
					"\t\tREF_DAY_TIME_temp T\r\n",
					"\t\tLEFT JOIN MAX_RPT_WEEK_IN_QTR MRW ON T.YEAR_NUM = MRW.YEAR_NUM AND T.QTR_NUM = MRW.QTR_NUM\r\n",
					"\t\tORDER BY 1\r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_WK_QTR_YEAR_BUS table\")\r\n",
					"\tdf_DIM_WK_QTR_YEAR_BUS.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_WK_QTR_YEAR_BUS\")\r\n",
					"\tlogger.info(\"Load Completed: time.DIM_WK_QTR_YEAR_BUS\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\r\n",
					"\r\n",
					"\t#Load TIME.DIM_MONTH_YEAR_BUS table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for TIME.DIM_MONTH_YEAR_BUS table\")\r\n",
					"\tdf_DIM_MONTH_YEAR_BUS = spark.sql(\"\"\"\r\n",
					"\t\tSELECT DISTINCT \r\n",
					"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 12) + T.MONTH_IN_FISCAL_YEAR_NUM) as integer) AS YEAR_MONTH_SEQ_WED,\r\n",
					"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
					"\t\tcast(T.MONTH_IN_FISCAL_YEAR_NUM as Integer) as MONTH_IN_FISCAL_YEAR_NUM,\r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
					"\t\tFROM \r\n",
					"\t\tREF_DAY_TIME_temp T\r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_MONTH_YEAR_BUS table\")\r\n",
					"\tdf_DIM_MONTH_YEAR_BUS.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_MONTH_YEAR_BUS\")\r\n",
					"\tlogger.info(\"Load Completed:  Time.DIM_MONTH_YEAR_BUS\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\t#Load TIME.DIM_QTR_YEAR_BUS table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for table time.DIM_QTR_YEAR_BUS\")\r\n",
					"\tdf_DIM_QTR_YEAR_BUS = spark.sql(\"\"\"\r\n",
					"\t\tSELECT DISTINCT \r\n",
					"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
					"\t\tcast(T.FISCAL_QTR_NUM as integer) as FISCAL_QTR_NUM,\r\n",
					"\t\tcast(T.FISCAL_YEAR_NUM as integer) as FISCAL_YEAR_NUM,\r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS\r\n",
					"\t\tFROM \r\n",
					"\t\tREF_DAY_TIME_temp T\r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_QTR_YEAR_BUS table\")\r\n",
					"\tdf_DIM_QTR_YEAR_BUS.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_YEAR_BUS\")\r\n",
					"\tlogger.info(\"Load Completed: time.DIM_QTR_YEAR_BUS\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\r\n",
					"\t#Load TIME.DIM_QTR_MNEMONIC_OFFSET table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for table time.DIM_QTR_MNEMONIC_OFFSET from DIM_QTR_MNEMONIC_OFFSET.csv\")\r\n",
					"\tmnemonic_file = adls_host + '/KYN_ISIP/time/raw/' + \"DIM_QTR_MNEMONIC_OFFSET.csv\"\r\n",
					"\tlogger.info(\"Reaing DIM_QTR_MNEMONIC_OFFSET.csv file\")\r\n",
					"\tdf_mnemonic_file = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(mnemonic_file).createOrReplaceTempView(\"DIM_QTR_MNEMONIC_OFFSET_TEMP\")\r\n",
					"\tdf_DIM_QTR_MNEMONIC_OFFSET = spark.sql(\"\"\"\r\n",
					"\t\tselect distinct\r\n",
					"\t\tcast((((T.FISCAL_YEAR_NUM - 1) * 4) + T.FISCAL_QTR_NUM) as integer) AS YEAR_QTR_SEQ_WED,\r\n",
					"\t\tcast(((T.YEAR_NUM - 1) * 4) + T.QTR_NUM AS integer) as YEAR_QTR_SEQ,\r\n",
					"\t\tcast(((T.YEAR_NUM) * 4 + T.QTR_NUM) - (YEAR(CURRENT_DATE) * 4 + QUARTER(CURRENT_DATE)) AS SMALLINT) AS DAILY_QTR_MNEMONIC_OFFSET,\r\n",
					"\t\tcast(((T.YEAR_NUM) * 4 + T.QTR_NUM) - (YEAR(CURRENT_DATE) * 4 + QUARTER(CURRENT_DATE)) AS SMALLINT) AS QTR_MNEMONIC_OFFSET,\r\n",
					"\t\tCASE \r\n",
					"\t\tWHEN YEAR(CURRENT_DATE) = T.YEAR_NUM AND QUARTER(CURRENT_DATE) = T.QTR_NUM THEN 'Y' \r\n",
					"\t\tELSE 'N' \r\n",
					"\t\tEND AS CURR_QTR_INDC,\r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS from REF_DAY_TIME_temp T\r\n",
					"\t\"\"\")\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_QTR_MNEMONIC_OFFSET table\")\r\n",
					"\tdf_DIM_QTR_MNEMONIC_OFFSET.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_QTR_MNEMONIC_OFFSET\")\r\n",
					"\tlogger.info(\"Load Completed: time.DIM_QTR_MNEMONIC_OFFSET\")\r\n",
					"\tlogger.info(\"--------------------------------------------------\")\r\n",
					"\r\n",
					"\t#Load TIME.DIM_MNEMONIC table.\r\n",
					"\tlogger.info(\"Executing SQL to get data for table time.DIM_MNEMONIC from DIM_MNEMONIC.csv\")\r\n",
					"\tmnemonic_file = adls_host + '/KYN_ISIP/time/raw/' + \"DIM_MNEMONIC.csv\"\r\n",
					"\tlogger.info(\"Reading DIM_MNEMONIC.csv file\")\r\n",
					"\tdf_mnemonic_file = sqlContext.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\", \"true\").load(mnemonic_file).createOrReplaceTempView(\"DIM_MNEMONIC_TEMP\")\r\n",
					"\r\n",
					"\tdf_DIM_MNEMONIC = spark.sql(\"\"\"\r\n",
					"\t\tselect \r\n",
					"\t\tcast(T.DIM_QTR_MNEMONIC_OFFSET as integer) as DIM_QTR_MNEMONIC_OFFSET,\r\n",
					"\t\tQTR_MNEMONIC_ID as QTR_MNEMONIC_ID,\r\n",
					"\t\tQTR_MNEMONIC_DESCR as QTR_MNEMONIC_DESCR,\r\n",
					"\t\tACTIVE_INDC as ACTIVE_INDC,\r\n",
					"\t\tCURRENT_TIMESTAMP AS IW_ROW_UPDT_TS from DIM_MNEMONIC_TEMP T\r\n",
					"\t\"\"\")\r\n",
					"\r\n",
					"\tlogger.info(\"Writing data into TIME.DIM_MNEMONIC table\")\r\n",
					"\tdf_DIM_MNEMONIC.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"time.DIM_MNEMONIC\")\r\n",
					"\tlogger.info(\"Load Completed: time.DIM_MNEMONIC\")\r\n",
					"\tlogger.info(\"------------------END OF TIME DIMENSION TABLE LOADS---------------\")\r\n",
					"\r\n",
					"if __name__ == \"__main__\":\r\n",
					"    main()\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": 34
			}
		]
	}
}